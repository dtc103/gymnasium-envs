{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "import gymnasium as gym\n",
    "#env = gym.make(\"ALE/Breakout-v5\", render_mode=\"human\")\n",
    "env = gym.make(\"ALE/Breakout-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, max_samples):\n",
    "        self.memory = deque([], maxlen=max_samples)\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, width, height, channels, output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=12, kernel_size=3, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.hidden1 = nn.Linear(int((24 * np.floor((width / 4)) * np.floor((height / 4)))), 265)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(265, 265)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.out = nn.Linear(265, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_size = env.action_space.n\n",
    "height, width, channels = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "action_size = env.action_space.n\n",
    "height, width, channels = env.observation_space.shape\n",
    "\n",
    "learning_rate = 0.005\n",
    "tau = 0.01\n",
    "gamma = 0.99\n",
    "replay_memory_size = 5000\n",
    "batch_size = 150\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "episodes = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_policy = DQN(width, height, channels, action_size)\n",
    "policy_net = DQN(width, height, channels, action_size)\n",
    "target_policy.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "memory = ReplayMemory(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, amsgrad=True)\n",
    "bellmann_error = nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, steps):\n",
    "    eps_max = 0.95\n",
    "    eps_min = 0.05\n",
    "    eps_step = 1000\n",
    "\n",
    "    threshold = eps_min + (eps_max - eps_min) * np.exp(-1 * (steps / eps_step))\n",
    "\n",
    "    selection = np.random.rand()\n",
    "\n",
    "    if selection > threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.item()\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def optimize():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    #there won't be None values in state, which is why we can cat it\n",
    "    states = torch.cat([t[0] for t in transitions])\n",
    "    actions = torch.tensor([t[1] for t in transitions]).unsqueeze(-1)\n",
    "    rewards = torch.tensor([t[3] for t in transitions])\n",
    "    \n",
    "    #there will be None values in next_state which is why we cant cat it -> create mask\n",
    "    next_states = [t[2] for t in transitions]\n",
    "    non_final_next_states_idxs = torch.tensor([i for i, t in enumerate(next_states) if t is not None])\n",
    "    non_final_next_states = torch.cat([t for t in next_states if t is not None])\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"duration: \", end-start)\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    q_values_policy_net = policy_net(states).gather(1, actions)\n",
    "    \n",
    "    q_values_next_states = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_values_next_states[non_final_next_states_idxs] = target_policy(non_final_next_states).max(1).values\n",
    "        \n",
    "    expected_q_values = rewards + (gamma * q_values_next_states)\n",
    "    \n",
    "    loss = bellmann_error(q_values_policy_net, expected_q_values.unsqueeze(-1))\n",
    "    \n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    print(\"loss: \", loss.item())\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    \n",
    "    policy_optimizer.step()\n",
    "    \n",
    "    print(\"Optimization and Inference time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Done\n",
      "------------------------------------------------------\n",
      "False\n",
      "1\n",
      "False\n",
      "2\n",
      "False\n",
      "3\n",
      "False\n",
      "4\n",
      "False\n",
      "5\n",
      "False\n",
      "6\n",
      "False\n",
      "7\n",
      "False\n",
      "8\n",
      "False\n",
      "9\n",
      "False\n",
      "10\n",
      "False\n",
      "11\n",
      "False\n",
      "12\n",
      "False\n",
      "13\n",
      "False\n",
      "14\n",
      "False\n",
      "15\n",
      "False\n",
      "16\n",
      "False\n",
      "17\n",
      "False\n",
      "18\n",
      "False\n",
      "19\n",
      "False\n",
      "20\n",
      "False\n",
      "21\n",
      "False\n",
      "22\n",
      "False\n",
      "23\n",
      "False\n",
      "24\n",
      "False\n",
      "25\n",
      "False\n",
      "26\n",
      "False\n",
      "27\n",
      "False\n",
      "28\n",
      "False\n",
      "29\n",
      "False\n",
      "30\n",
      "False\n",
      "31\n",
      "False\n",
      "32\n",
      "False\n",
      "33\n",
      "False\n",
      "34\n",
      "False\n",
      "35\n",
      "False\n",
      "36\n",
      "False\n",
      "37\n",
      "False\n",
      "38\n",
      "False\n",
      "39\n",
      "False\n",
      "40\n",
      "False\n",
      "41\n",
      "False\n",
      "42\n",
      "False\n",
      "43\n",
      "False\n",
      "44\n",
      "False\n",
      "45\n",
      "False\n",
      "46\n",
      "False\n",
      "47\n",
      "False\n",
      "48\n",
      "False\n",
      "49\n",
      "False\n",
      "50\n",
      "False\n",
      "51\n",
      "False\n",
      "52\n",
      "False\n",
      "53\n",
      "False\n",
      "54\n",
      "False\n",
      "55\n",
      "False\n",
      "56\n",
      "False\n",
      "57\n",
      "False\n",
      "58\n",
      "False\n",
      "59\n",
      "False\n",
      "60\n",
      "False\n",
      "61\n",
      "False\n",
      "62\n",
      "False\n",
      "63\n",
      "False\n",
      "64\n",
      "False\n",
      "65\n",
      "False\n",
      "66\n",
      "False\n",
      "67\n",
      "False\n",
      "68\n",
      "False\n",
      "69\n",
      "False\n",
      "70\n",
      "False\n",
      "71\n",
      "False\n",
      "72\n",
      "False\n",
      "73\n",
      "False\n",
      "74\n",
      "False\n",
      "75\n",
      "False\n",
      "76\n",
      "False\n",
      "77\n",
      "False\n",
      "78\n",
      "False\n",
      "79\n",
      "False\n",
      "80\n",
      "False\n",
      "81\n",
      "False\n",
      "82\n",
      "False\n",
      "83\n",
      "False\n",
      "84\n",
      "False\n",
      "85\n",
      "False\n",
      "86\n",
      "False\n",
      "87\n",
      "False\n",
      "88\n",
      "False\n",
      "89\n",
      "False\n",
      "90\n",
      "False\n",
      "91\n",
      "False\n",
      "92\n",
      "False\n",
      "93\n",
      "False\n",
      "94\n",
      "False\n",
      "95\n",
      "False\n",
      "96\n",
      "False\n",
      "97\n",
      "False\n",
      "98\n",
      "False\n",
      "99\n",
      "False\n",
      "100\n",
      "False\n",
      "101\n",
      "False\n",
      "102\n",
      "False\n",
      "103\n",
      "False\n",
      "104\n",
      "False\n",
      "105\n",
      "False\n",
      "106\n",
      "False\n",
      "107\n",
      "False\n",
      "108\n",
      "False\n",
      "109\n",
      "False\n",
      "110\n",
      "False\n",
      "111\n",
      "False\n",
      "112\n",
      "False\n",
      "113\n",
      "False\n",
      "114\n",
      "False\n",
      "115\n",
      "False\n",
      "116\n",
      "False\n",
      "117\n",
      "False\n",
      "118\n",
      "False\n",
      "119\n",
      "False\n",
      "120\n",
      "False\n",
      "121\n",
      "False\n",
      "122\n",
      "False\n",
      "123\n",
      "False\n",
      "124\n",
      "False\n",
      "125\n",
      "False\n",
      "126\n",
      "False\n",
      "127\n",
      "False\n",
      "128\n",
      "False\n",
      "129\n",
      "False\n",
      "130\n",
      "False\n",
      "131\n",
      "False\n",
      "132\n",
      "False\n",
      "133\n",
      "False\n",
      "134\n",
      "False\n",
      "135\n",
      "False\n",
      "136\n",
      "False\n",
      "137\n",
      "False\n",
      "138\n",
      "False\n",
      "139\n",
      "False\n",
      "140\n",
      "False\n",
      "141\n",
      "False\n",
      "142\n",
      "False\n",
      "143\n",
      "False\n",
      "144\n",
      "False\n",
      "145\n",
      "False\n",
      "146\n",
      "False\n",
      "147\n",
      "False\n",
      "148\n",
      "False\n",
      "149\n",
      "False\n",
      "duration:  0.03534865379333496\n",
      "loss:  0.006455219816416502\n",
      "Optimization and Inference time:  3.8426520824432373\n",
      "150\n",
      "False\n",
      "duration:  0.056845664978027344\n",
      "loss:  0.006455219816416502\n",
      "Optimization and Inference time:  1.799971580505371\n",
      "151\n",
      "False\n",
      "duration:  0.02915191650390625\n",
      "loss:  0.006437213625758886\n",
      "Optimization and Inference time:  1.7279186248779297\n",
      "152\n",
      "False\n",
      "duration:  0.030149459838867188\n",
      "loss:  0.006480421405285597\n",
      "Optimization and Inference time:  1.533952236175537\n",
      "153\n",
      "False\n",
      "duration:  0.02606368064880371\n",
      "loss:  0.006420555058866739\n",
      "Optimization and Inference time:  2.097801923751831\n",
      "154\n",
      "False\n",
      "duration:  0.05092501640319824\n",
      "loss:  0.006517310161143541\n",
      "Optimization and Inference time:  1.943664312362671\n",
      "155\n",
      "False\n",
      "duration:  0.03131103515625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m state \u001b[38;5;241m=\u001b[39m new_state\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# optimize step here\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# until here\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m#update weights of target with policy\u001b[39;00m\n\u001b[1;32m     34\u001b[0m target_dic \u001b[38;5;241m=\u001b[39m target_policy\u001b[38;5;241m.\u001b[39mstate_dict()\n",
      "Cell \u001b[0;32mIn[57], line 25\u001b[0m, in \u001b[0;36moptimize\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration: \u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m-\u001b[39mstart)\n\u001b[1;32m     23\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m q_values_policy_net \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m, actions)\n\u001b[1;32m     27\u001b[0m q_values_next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[51], line 18\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool1(x)\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    state = (torch.tensor(state, device=device).float() / 255).permute(2, 0, 1).unsqueeze(0)\n",
    "    \n",
    "    done = False\n",
    "    step = 0\n",
    "    \n",
    "    print(\"Episode Done\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state, step)\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        print(done)\n",
    "\n",
    "        if terminated:\n",
    "            new_state = None\n",
    "        else:\n",
    "            new_state = (torch.tensor(new_state, device=device).float() / 255).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, new_state, reward)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        # optimize step here\n",
    "        optimize()\n",
    "        # until here\n",
    "\n",
    "        #update weights of target with policy\n",
    "        target_dic = target_policy.state_dict()\n",
    "        policy_dic = policy_net.state_dict()\n",
    "\n",
    "        for keys in target_dic:\n",
    "            target_dic[keys] = policy_dic[keys] * tau + target_dic[keys] * (1-tau)\n",
    "\n",
    "        target_policy.load_state_dict(target_dic)\n",
    "\n",
    "        step += 1\n",
    "        print(step)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
