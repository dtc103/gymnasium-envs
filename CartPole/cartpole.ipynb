{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import random\n",
    "from collections import deque\n",
    "\t\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "#env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "env._max_episode_steps = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, max_samples):\n",
    "        self.memory = deque([], maxlen=max_samples)\n",
    "\n",
    "    def push(self, state, action, next_state, reward):\n",
    "        self.memory.append((state, action, next_state, reward))\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden1 = nn.Linear(input_size, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(256, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.out = nn.Linear(256, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.hidden2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_observations = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "learning_rate = 0.005\n",
    "gamma = 0.99\n",
    "tau = 0.01\n",
    "replay_memory_size = 5000\n",
    "batch_size = 100\n",
    "\n",
    "episodes = 1200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_policy = DQN(n_observations[0], n_actions)\n",
    "policy_net = DQN(n_observations[0], n_actions)\n",
    "target_policy.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "memory = ReplayMemory(replay_memory_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate, amsgrad=True)\n",
    "bellmann_error = nn.HuberLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, steps):\n",
    "    eps_max = 0.95\n",
    "    eps_min = 0.05\n",
    "    eps_step = 10000\n",
    "\n",
    "    threshold = eps_min + (eps_max - eps_min) * np.exp(-1 * (steps / eps_step))\n",
    "\n",
    "    selection = np.random.rand()\n",
    "\n",
    "    if selection > threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1).indices.item()\n",
    "    else:\n",
    "        return env.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "episode_durations = []\n",
    "cum_rewards = []\n",
    "\n",
    "def plot_durations(show_result=False):\n",
    "    plt.subplots\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    rewards_t = torch.tensor(cum_rewards, dtype=torch.float32)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    plt.plot(rewards_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    \n",
    "    transitions = memory.sample(batch_size)\n",
    "\n",
    "    #there won't be None values in state, which is why we can cat it\n",
    "    states = torch.cat([t[0] for t in transitions])\n",
    "    actions = torch.tensor([t[1] for t in transitions]).unsqueeze(-1)\n",
    "    rewards = torch.tensor([t[3] for t in transitions], dtype=torch.float32)\n",
    "    \n",
    "    #there will be None values in next_state which is why we cant cat it -> create mask\n",
    "    next_states = [t[2] for t in transitions]\n",
    "    non_final_next_states_idxs = torch.tensor([i for i, t in enumerate(next_states) if t is not None])\n",
    "    non_final_next_states = torch.cat([t for t in next_states if t is not None])\n",
    "    \n",
    "    q_values_policy_net = policy_net(states).gather(1, actions)\n",
    "    \n",
    "    q_values_next_states = torch.zeros(batch_size, dtype=torch.float32, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        q_values_next_states[non_final_next_states_idxs] = target_policy(non_final_next_states).max(1).values\n",
    "        \n",
    "    expected_q_values = rewards + (gamma * q_values_next_states)\n",
    "    \n",
    "    loss = bellmann_error(q_values_policy_net, expected_q_values.unsqueeze(-1))\n",
    "    \n",
    "    policy_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    \n",
    "    policy_optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "step = 0\n",
    "\n",
    "for episode in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    done = False\n",
    "    \n",
    "    cum_rew = 0.0\n",
    "    t = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = choose_action(state, step)\n",
    "\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        # decrease the reward, the more it deviates from the center\n",
    "        reward = reward * (1 - np.abs(new_state[0] / 4.8)) #4.8 is the maximum observation in each direction\n",
    "        cum_rew += reward\n",
    "\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if terminated:\n",
    "            new_state = None\n",
    "            cum_rew -= 500\n",
    "        else:\n",
    "            new_state = torch.tensor(new_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, new_state, cum_rew)\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "        # optimize step here\n",
    "        optimize()\n",
    "        # until here\n",
    "\n",
    "        #update weights of target with policy\n",
    "        target_dic = target_policy.state_dict()\n",
    "        policy_dic = policy_net.state_dict()\n",
    "\n",
    "        for keys in target_dic:\n",
    "            target_dic[keys] = policy_dic[keys] * tau + target_dic[keys] * (1-tau)\n",
    "\n",
    "        target_policy.load_state_dict(target_dic)\n",
    "        \n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "\n",
    "        step += 1\n",
    "        t+=1\n",
    "    \n",
    "    cum_rewards.append(cum_rew)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(target_policy.state_dict(), \"cartpole_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_net = DQN(n_observations[0], n_actions)\n",
    "state_dict = torch.load(\"cartpole_target\")\n",
    "evaluation_net.load_state_dict(state_dict)\n",
    "\n",
    "evaluation_net.eval()\n",
    "state, _ = env.reset()\n",
    "state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = choose_action(state, step)\n",
    "\n",
    "    new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "    done = terminated or truncated\n",
    "    \n",
    "    if terminated:\n",
    "        new_state = None\n",
    "        cum_rew -= 500\n",
    "    else:\n",
    "        new_state = torch.tensor(new_state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "    state = new_state\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
